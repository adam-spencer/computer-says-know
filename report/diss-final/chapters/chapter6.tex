\chapter{Results and Discussion}\label{ch:results-and-discussion}

\mycomment{
  * Chapter introduction
  * Results section
    > simulate a manual transcriber correcting results by ordering utterances using some metric and using specific WER caclulation
    > Run as average for single conversations AND whole corpus
    > present results using word-confidence
    > could make a table to show percentage of data checked versus reduction in WER?
  * Discussion section
    > average_logprob and utterance confidence are similar- appear to function well for average convo but poorly for whole corpus
    > explain: utterance confidence is calculated from the same logit data as utterance confidence (thus have similar shape)
    > explain word-confidence
    > works much better - long utterances can have few low-confidence words but get high average, word-confidence ordering eliminates this
    > word-confidence seems to work! there is benefit from using this metric
    > other two not so much
    > is this test representative of a real user?
    > by showing that there is a confidence metric which seems to work, Whisper is demonstrated to be a viable choice for aiding a human transcriber
    > perhaps work through requirements?
  * Future work (maybe a better title?)
    > try different type of confidence scoring - perhaps Whisper isn't the best candidate due to the way it makes choices?
    > try different datasets - LifeLUCID is only one out of thousand of corpora, try to replicate results
    > make fully-working transcription software - my example only shows the effect of ordering results, doesn't allow input
    > compare the effect of different model sizes - errors may be clustered differently - if smaller models could reliably get confidence then the hardware and time required to generate transcripts would be minimised
}

\section{Simulating Computer-Aided Transcription}\label{sec:simulating-transcription}

An effective computer-aided transcription system must reduce the human cost required to lower the amount of errors a transcription to an acceptable level.
By ordering transcribed utterances using some confidence metric and letting a human transcriber make corrections, an effective system should see the error rate fall more rapidly than if the results were corrected in a random order. \\

A simple simulation was devised in order to test the effectiveness of each potential measure of system confidence by using the formula for WER as follows;

\begin{enumerate}
  \item For a set of $M$ utterances $U = \{ u_{0}, u_{1}, \ldots\, u_{M} \}$ (either a single conversation or entire corpus), the subsitutions ($S_i$), deletions ($D_i$), insertions ($I_i$), and number of words in the reference ($N_i$) are calculated for each utterance, $u_i$.
  \item $U$ is ordered using a confidence metric.
  \item The WER, $w_i$, is calculated for a slice of $U$ containing all items including and following some utterance, $u_i$;
    \[
      w_i = \frac{\sum_{j=i}^{M} (S_j + D_j + I_j)}{\sum_{k=0}^{M} N_k}
    \]
  \item Increment $i$ by $1$ then repeat the previous step for all $0 \leq i \leq M$.
    Notice that the denominator is the same for all slices but the numerator changes to simulate each utterance being corrected in order.
  \item Output is a set $W = \{ w_{0}, w_{1}, \ldots\, w_{M} \}$, where some item $w_{i}$ is equal to the WER of $U$ after correcting all utterances prior to $u_i$.
\end{enumerate}

These results shall be analysed by plotting graphs of WER against the percentage of utterance which have been human-corrected, henceforth refered to as 'Cost'.

\section{Simulation Results}

This section details the results acquired from running this simple simulation using results ordered with each of the following metrics;

\begin{itemize}
  \item \texttt{avg\_logprob} taken directly from Whisper's standard output;
  \item utterance-average confidence score; and
  \item utterance-minimum and -maximum word-confidence scores.
\end{itemize}

Where \say{utterance-minimum and -maximum word-confidence} refers to an ordering of utterances based on each utterances minimum and maximum word-level confidence score.

Results which show a per-conversation average have a shaded section to show the standard deviation from the mean, where the mean is the coloured line on the graph.

Dashed 'random order' lines show the WER/Cost trend expected if the results were manually corrected in a random order.
A performant metric for ordering results should have a plot showing a line which dips below the 'random order' line.
A metric which follows the trend of the 'random order' line is thus performing the same as if the results were randomly ordered and therefore ineffective for computer-aided transcription.

\include{figures/results_plots}

\include{tables/halve-wer-results}

\clearpage
\section{Discussion of Results}

This section shall discuss findings which can be drawn from these results.

\subsection{Utterance-average confidence and \texttt{avg\_logprob}}

The results show that \texttt{avg\_logprob} and utterance-average confidence perform relatively similarly on average for a given conversation (fig. \ref{fig:compare-avg-uttconf-vs-lprob}), with similarly poor performance when used to order the entire corpus (fig. \ref{fig:corpus-avg-lprob-uttconf}).
Utterance-average confidence appears to have marginally superior performance in both comparisons shown in section \ref{subsec:avg-word-conf}.

\mycomment{
  need to explain how conf and lprob are calculated in implementation or design section
}
The similarity between the plots of the utterance-average word confidence and Whisper's \texttt{avg\_logprob} output is likely due to their similarity in calculation.
Confidence is calculated by \emph{whisper-timestamped}\cite{whisper-timestamped} from the average of the word-level confidence scores in an utterance, which are calculated from the average of the sub-word log-probabilities for each word.
Whisper\cite{whisper} calculates \texttt{avg\_logprob} as simply the average of the log-probabilities of all tokens in an utterance.
Perhaps the slight performance increase seen when using utterance-average confidence scores could be explained due to it being calculated from word-level scores rather than token-level and the accuracy metric operating at the word-level rather than token-level too.

\subsection{Word-level confidence metrics}

The results presented in section \ref{subsec:avg-word-conf} show the per-conversation average relationship between WER and Cost for the non-ascending and non-descending utterance-minimum and -maximum word-confidence metrics.

It is apparent that for the utterance-minimum word-confidence metric it is best to use non-descending (fig. \ref{fig:word-conf-comparison-plot1}) rather than non-ascending order (fig. \ref{fig:word-conf-comparison-plot2}), as non-ascending order closely follows the random order (i.e. is ineffective).
This makes intuitive sense because ordering results on non-ascending minimum confidence would mean correcting the most confident results first, thus the opposite of an effective ordering method.

Notice that non-descending utterance-minimum word-confidence ordering (fig. \ref{fig:word-conf-comparison-plot1}) has the narrowest shaded area of all the plots presented, meaning it has the lowest standard deviation from the mean and therefore the most reliable representation of performance on a given conversation.

As for using utterance-maximum word-confidence ordering, non-ascending order (fig. \ref{fig:word-conf-comparison-plot4}) shows superior performance to non-descending (fig. \ref{fig:word-conf-comparison-plot3}).
The reason for this is less intuitive; it would make more sense that ordering from lowest maximum confidence to highest (i.e. non-descending) would put utterances with lower confidence scores first.
This could be due to word-confidence being derived from word-level probability scores, though this is an area which needs more experimentation to better understand this result.

When comparing whole-corpus evaluation results using word-confidence measures (fig. \ref{fig:corpus-all-word-conf-measures}), non-descending utterance-minimum word-confidence ordering has the best (largest difference from random) and most consistent (smoothest line) performance of all the metrics, followed by non-ascending utterance-maximum word-confidence.
The other two metrics are shown to be a hinderance, achieving worse performance than if the results were evaluated in random order and should thus be ignored as metrics to use in a computer-aided transcription system.

\subsection{Comparing word-level and utterance-level metrics}

Figure \ref{fig:corpus-allmeasures} presents a comparison between the performance of each metric when evaluating the whole corpus.
The metric with the best performance is clearly non-descending utterance-minimum word-confidence; it is much more consistent than the others and remains much further from random ordering, meaning it has the highest WER reduction for the same cost as all other metrics.

Figure \ref{fig:range-ordering-vs-stddev-vs-min-wconf} shows the performance of non-ascending ordering using the both range scores and standard deviation of word-confidence for each utterance. 
Notice the slight performance gains over non-descending utterance-minimum word-confidence.

These metrics show slight performance gains over non-descending utterance-minimum word-confidence, as illustrated in figure \ref{fig:delta-wer} which plots the difference in WER from using random ordering ($\Delta$WER) against Cost.

Table \ref{table:halve-wer-results} presents the cost required to halve WER, with non-ascending word-confidence range being the best performer.
According to these results, a computer-aided transcription system using this metric would require only 28\% of the results to be checked in order to halve the WER of the ASR output, in this case falling to approximately 6\% WER.
Considering that the corpus has been segmented into 7312 utterances, fewer than 2050 of them would require manually correcting to reduce the WER by half when ordered using this metric.
For reference, to achieve this result by correcting the results in random order would require checking over 3600 pieces of audio, or 44\% more.

\section{Future Work}

This section shall highlight some of the areas which this project has failed to adequately address and present potential avenues for further work.

\subsection{Increase test corpus}

This work has focused on only one speech corpus, \emph{LifeLUCID}\cite{lifelucid}.
Though it presents a diverse range of speaker ages, it has made for a relatively small test sample (less than 9 hours total).
Thorough testing on a variety of speech corpora would provide validatation to the results presented earlier in this chapter.
Other languages could be used to increase the diversity of the test corpus because Whisper is capable of operating on various different languages (though is trained on 65\% English data\cite{whisper}).

Despite the limited test corpus used in this work, the results of ordering utterances using metrics based on word-level confidence scores show a very clear benefit to the efficiency of a semi-automatic transcription system.
This benefit may differ in magnitude across corpora, though it seems highly unlikely that it would not provide a benefit to other similar corpora (i.e. consisting of English speakers).

\subsection{Acquire results from human transcribers}

The rudimentary simulation presented in section \ref{sec:simulating-transcription} is built on the assumption that a result is free from errors once human-corrected.
In reality, this is not always the case; humans often make mistakes, and therefore would produce different results from what was presented in this chapter.

An experiment using human participants would require a working computer-aided transcription software; the software demonstration presented for this project is not fully-featured and would require a small amount of modification before it could be used for transcription.
Specifically, it is not capable of taking text input from the user, though it can order, highlight and blank-out utterances, as well as play accompanying audio files.

\subsection{Experiment with other derivations of confidence}

This work has used confidence scoring derived from a single unmodified output from Whisper (\texttt{avg\_logprob}) and internal probability scoring.
While ordering utterances based on minimum word-level confidence scores derived from internal probability scoring has shown a degree of effectiveness, there is expansive literature discussing methods of neural network confidence measures which have not been implemented in this work.

For example, use of a neural network (e.g. a multilayer perceptron) which takes Whisper's output as input and outputs a confidence metric could be shown to provide a reliable metric for confidence.
Similar methods are proposed in the literature, such as using recurrent neural networks\cite{huang2013predicting, kalgaonkar2015estimating} or a na\"{i}ve Bayes classifier\cite{sanchis2011word}.
The benefit of this method would be that Whisper doesn't require any (or minimal) modification to apply them.

To avoid spending time experimenting with modifications to Whisper's architecture, a mildly modified version of Whisper called \emph{whisper-timestamped}\cite{whisper-timestamped} was used in this project.
An 'entropy-based' approach for calculating word-confidence is proposed in the literature\cite{laptev2023fast, qiu2021learning}, though implementation would require considerable modification to the Transformer which Whisper uses.

A solution to this problem may be to use a different model than Whisper, though at the time of writing there doesn't appear to be another free and open-source ASR model with similar performance.

