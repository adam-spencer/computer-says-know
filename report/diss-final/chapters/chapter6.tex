\chapter{Results and Discussion}\label{ch:results-and-discussion}

\mycomment{
  * Chapter introduction
  * Results section
    > simulate a manual transcriber correcting results by ordering utterances using some metric and using specific WER caclulation
    > Run as average for single conversations AND whole corpus
    > present results using word-confidence
    > could make a table to show percentage of data checked versus reduction in WER?
  * Discussion section
    > average_logprob and utterance confidence are similar- appear to function well for average convo but poorly for whole corpus
    > explain: utterance confidence is calculated from the same logit data as utterance confidence (thus have similar shape)
    > explain word-confidence
    > works much better - long utterances can have few low-confidence words but get high average, word-confidence ordering eliminates this
    > word-confidence seems to work! there is benefit from using this metric
    > other two not so much
    > is this test representative of a real user?
    > by showing that there is a confidence metric which seems to work, Whisper is demonstrated to be a viable choice for aiding a human transcriber
    > perhaps work through requirements?
  * Future work (maybe a better title?)
    > try different type of confidence scoring - perhaps Whisper isn't the best candidate due to the way it makes choices?
    > try different datasets - LifeLUCID is only one out of thousand of corpora, try to replicate results
    > make fully-working transcription software - my example only shows the effect of ordering results, doesn't allow input
    > compare the effect of different model sizes - errors may be clustered differently - if smaller models could reliably get confidence then the hardware and time required to generate transcripts would be minimised
}

\section{Simulating Computer-Aided Transcription}

An effective computer-aided transcription system must reduce the human cost required to lower the amount of errors a transcription to an acceptable level.
By ordering transcribed utterances using some confidence metric and letting a human transcriber make corrections, an effective system should see the error rate fall more rapidly than if the results were corrected in a random order. \\

A simple simulation was devised in order to test the effectiveness of each potential measure of system confidence by using the formula for WER as follows;

\begin{enumerate}
  \item For a set of $M$ utterances $U = \{ u_{0}, u_{1}, \ldots\, u_{M} \}$ (either a single conversation or entire corpus), the subsitutions ($S_i$), deletions ($D_i$), insertions ($I_i$), and number of words in the reference ($N_i$) are calculated for each utterance, $u_i$.
  \item $U$ is ordered using a confidence metric.
  \item The WER, $w_i$, is calculated for a slice of $U$ containing all items including and following some utterance, $u_i$;
    \[
      w_i = \frac{\sum_{j=i}^{M} (S_j + D_j + I_j)}{\sum_{k=0}^{M} N_k}
    \]
  \item Increment $i$ by $1$ then repeat the previous step for all $0 \leq i \leq M$.
    Notice that the denominator is the same for all slices but the numerator changes to simulate each utterance being corrected in order.
  \item Output is a set $W = \{ w_{0}, w_{1}, \ldots\, w_{M} \}$, where some item $w_{i}$ is equal to the WER of $U$ after correcting all utterances prior to $u_i$.
\end{enumerate}

These results shall be analysed by plotting graphs of WER against the percentage of utterance which have been human-corrected, henceforth refered to as 'Cost'.

\section{Simulation Results}

This section details the results acquired from running this simple simulation using results ordered with each of the following metrics;

\begin{itemize}
  \item \texttt{avg\_logprob} taken directly from Whisper's standard output;
  \item utterance-average confidence score; and
  \item utterance-minimum and -maximum word-confidence scores.
\end{itemize}

Where \say{utterance-minimum and -maximum word-confidence} refers to an ordering of utterances based on each utterances minimum and maximum word-level confidence score.

Results which show a per-conversation average have a shaded section to show the standard deviation from the mean, where the mean is the coloured line on the graph.

Dashed 'random order' lines show the WER/Cost trend expected if the results were manually corrected in a random order.
A performant metric for ordering results should have a plot showing a line which dips below the 'random order' line.
A metric which follows the trend of the 'random order' line is thus performing the same as if the results were randomly ordered and therefore ineffective for computer-aided transcription.

\include{figures/results_plots}

\clearpage
\section{Discussion of Results}

This section shall discuss findings which can be drawn from these results.

\subsection{Utterance-average confidence and \texttt{avg_logprob}}

The results show that \texttt{avg\_logprob} and utterance-average confidence perform relatively similarly on average for a given conversation (\ref{fig:compare-avg-uttconf-vs-lprob}), with similarly poor performance when used to order the entire corpus (\ref{fig:corpus-avg-lprob-uttconf}).
Utterance-average confidence appears to have marginally superior performance in both comparisons shown in section \ref{subsec:avg-word-conf}.

\mycomment{
  need to explain how conf and lprob are calculated in implementation or design section
}
The similarity between the plots of the utterance-average word confidence and Whisper's \texttt{avg\_logprob} output is likely due to their similarity in calculation.
Confidence is calculated by \emph{whisper-timestamped}\cite{whisper-timestamped} from the average of the word-level confidence scores in an utterance, which are calculated from the average of the sub-word log-probabilities for each word.
Whisper\cite{whisper} calculates \texttt{avg\_logprob} as simply the average of the log-probabilities of all tokens in an utterance.
Perhaps the slight performance increase seen when using utterance-average confidence scores could be explained due to it being calculated from word-level scores rather than token-level and the accuracy metric operating at the word-level rather than token-level too.

\subsection{Word-level confidence metrics}

The results presented in section \ref{subsec:avg-word-conf}

\subsection{Comparing word-level and utterance-level metrics}

\section{Future Work}

This section shall suggest areas which may benefit from further work in the future which have become clear while completing this work.
