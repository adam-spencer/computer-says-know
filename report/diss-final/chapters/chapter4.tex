\chapter{Design}\label{ch:design}

\section{Speech Data}

According to its authors, Whisper's robustness is due likely in part to its use of a language model in its decoder\cite{whisper}.
Though likely beneficial for keeping track of sentence context, this poses a potential threat to the models accuracy in a number of circumstances, including;

\begin{itemize}
  \item Misspoken words or sentences with improper syntax (e.g. 'then' instead of 'than'), for these errors may be corrected by the model, despite being innacurate to the original recording.
  \item Disjoint terms (e.g. 'book purple dishsoap'), as such terms are highly unlikely to occur in sequence and thus the language model will not consider them a probable output.
\end{itemize}

To combat these drawbacks, this work will use a conversational speech corpus rather than one made of spoken disjoint terms.
While not representative of all speech, an argument can be made that the majority of speech which must be transcribed (e.g. conference recordings, courtroom hearings, lectures, etc.) has a maintained context throughout and is not significantly formed of disjoint terms, though the potential for disjoint terms to be present in a recording should be acknowledged as a potential area of weakness for Whisper.

To introduce the chosen corpus, it is important to state that the original aim of this work was to explore the impact of age-related changes to speech on ASR performance, and while researching for this project the \emph{LifeLUCID} corpus\cite{lifelucid} was determined to be the best match.
Despite the scope of the project having since changed, the data gathered fits the new objective very well, as it is formed of 52 recordings of conversations between 104 discrete speakers aged between 8 and 85 years old.
They are solving a 'spot-the-difference' task, and the data selected for this work was recorded in normal conditions (that is, they can hear and communicate with eachother normally).

The corpus' authors mention that the reference transcripts were generated by an ASR system and only one channel's audio was human-corrected.
The ability to compare the quality of ASR output to a reference transcript is required to evaluate this work, thus, only the human-corrected transcript and corresponding audio channel were used to ensure the references are reliable.
This leaves 52 10-minute recordings of individual speakers with gaps where the other participant is speaking.

\subsection{TextGrid Format}

The reference transcripts are supplied in \emph{Praat TextGrid} format which is produced by the Praat software suite\cite{praat}.
The TextGrid format consists of each individual part of speech (words, hesitations, mid-sentence silences, silences when the other participant is speaking etc.) being present in consecutive entries with the time in the recording which they start and finish.

This is an example of a single word in TextGrid format;

\begin{figure}[h!]
\centering
\begin{BVerbatim}
intervals [14]:
  xmin = 21.05 
  xmax = 21.47 
  text = "BUSH" 
\end{BVerbatim}
\end{figure}

A full example of this format is available in Appendix \ref{appendix:textgrid}.

As shown by this example, \texttt{xmin} and \texttt{xmax} denote the points in the recording at which the section starts and ends, and \texttt{text} denotes the content of the section.
Considering that each entry appears consecutively and that there are over 1,000 in each file, the format is not easily human-readable.

\subsection{Data Preparation}

There are a number of issues with using the data in its original format, including;

\begin{enumerate}
  \item Whisper struggles to maintain alignment when transcribing longform data\cite{whisper}, so the approximate 10-minute length of each recording requires shortening to maintain system performance;
  \item TextGrids are not human-readable; and
  \item The output of the ASR system should be stored alongside the reference transcripts to ease evaluation, which is not possible using TextGrids.
\end{enumerate}

The solution to the first problem would best be solved by splitting the conversation recordings into individual utterances.
Luckily, the human-evaluated references include metadata which shows the start- and stop-times of each word and non-word part of speech, meaning it can easily be split into individual utterances without using voice activation detection or other automatic techniques.
Using the documentation for LifeLUCID it was possible to determine that there are two types of non-speech token;

\begin{enumerate}
  \item 'Break' tokens -- these are tokens which denote the speaker is not mid-utterance; either listening to the other participant or engaged in irrelevant discussion (these latter parts are silenced in the recordings).
  \item 'Junk' tokens -- these denote either:
    \begin{itemize}
      \item The speaker has paused (but the other participant is not speaking).
      \item A bell or dog bark is being played as part of their task (these are silent in the recording).
      \item Hesitations (e.g., 'umm', 'uhhh', etc.)
      \item Other non-speech, non-breaking tokens (not specified in their documentation but present in the transcripts).
    \end{itemize}
\end{enumerate}

For the purpose of this work, an utterance is defined as an uninterrupted piece of speech without any long pauses.
By providing threshold values for the minimum length of a 'break' token and maximum length of a 'junk' token, the boundaries at which utterances start and end can be easily computed from the reference TextGrids.
The utterance boundaries can then be used to extract individual utterances from the longform recordings, resulting in a series of numbered audio files.

The second and third problems can be solved together by changing from the TextGrid format to JSON (JavaScript Object Notation).
This format is human-readable\cite{nurseitov2009comparison} and able to hold all the data and metadata required for this work, including a way to reference the original piece of audio it represents.

\section{Running Whisper}

\mycomment{
  Would it be worth moving some of this to chap3? defining a 'target user' is probably better suited to there...
  }
Whisper comes packaged with models of various sizes, requiring between approximately 1 and 10GB of VRAM and an increasing amount of time to produce transcripts.
Considering the objective to create an automatic transcription system which uses entirely free and open-source software, it is worth assuming that the individuals or institutions who would benefit the most from this system are those without the resources to rely on professional manual transcription.
It follows, then, that these 'target users' would not have access to high-powered computers and instead rely on consumer-grade hardware to generate transcriptions.
Thus, for the purposes of this work, the medium English-language model was selected as it requires only 5GB of VRAM and takes approximately half as much time to produce transcripts as the larger models\cite{whisper}.

\subsection{High-Powered Computing}

\section{Confidence Scoring}

Core to this work is the need for a reliable and accurate measure of ASR confidence.

\include{tables/confidence-motivations}

\section{Conceptual Transcription Systems}

