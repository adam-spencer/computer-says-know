\chapter{Design}\label{ch:design}

\section{Speech Data}

According to its authors, Whisper's robustness is due likely in part to its use of a language model in its decoder\cite{whisper}.
Though likely beneficial for keeping track of sentence context, this poses a potential threat to the models accuracy in a number of circumstances, including;

\begin{itemize}
  \item Misspoken words or sentences with improper syntax (e.g. 'then' instead of 'than'), for these errors may be corrected by the model, despite being inaccurate to the original recording.
  \item Disjoint terms (e.g. 'book purple dish soap'), as such terms are highly unlikely to occur in sequence and thus the language model will not consider them a probable output.
\end{itemize}

To combat these drawbacks, this work will use a conversational speech corpus rather than one made of spoken disjoint terms.
While not representative of all speech, an argument can be made that the majority of speech which must be transcribed (e.g. conference recordings, courtroom hearings, lectures, etc.) has a maintained context throughout and is not significantly formed of disjoint terms, though the potential for disjoint terms to be present in a recording should be acknowledged as a potential area of weakness for Whisper.

\mycomment{
  Change this, sounds jank
  }
While introducing the corpus selected for this work that the original objective was to explore the impact of age-related changes to speech on ASR performance, and for this goal the \emph{LifeLUCID} corpus\cite{lifelucid} was determined to be the best match.
Despite the scope of the project having since changed, the data gathered fits the new objective very well, as it is formed of 52 recordings of conversations between 104 discrete speakers aged between 8 and 85 years old.
They are solving a 'spot-the-difference' task, and the data selected for this work was recorded in normal conditions (that is, they can hear and communicate with each other normally).

The corpus' authors mention that the reference transcripts were generated by an ASR system and only one channel's audio was human-corrected.
The ability to compare the quality of ASR output to a reference transcript is required to evaluate this work, thus, only the human-corrected transcript and corresponding audio channel were used to ensure the references are reliable.
This leaves 52 10-minute recordings of individual speakers with gaps where the other participant is speaking.

\subsection{TextGrid Format}

The reference transcripts are supplied in \emph{Praat TextGrid} format which is produced by the Praat software suite\cite{praat}.
The TextGrid format consists of each individual part of speech (words, hesitations, mid-sentence silences, silences when the other participant is speaking etc.) being present in consecutive entries with the time in the recording which they start and finish.

\begin{figure}[h!]
\centering
\begin{BVerbatim}
intervals [14]:
  xmin = 21.05 
  xmax = 21.47 
  text = "BUSH" 
\end{BVerbatim}
  \caption{Example of an entry in TextGrid format}
  \label{fig:textgrid-example}
\end{figure}

Figure \ref{fig:textgrid-example} is an example of a single 'interval' in the TextGrid format.
A larger example is available in Appendix \ref{appendix:textgrid}
You may observe that \texttt{xmin} and \texttt{xmax} denote the points in the recording at which the section starts and ends, and \texttt{text} denotes the content of the section.
Considering that each entry appears consecutively and that there are over 1,000 in each file, the format is not easily human-readable.

\subsection{Data Preparation}

There are a number of issues with using the data in its original format, including;

\begin{enumerate}
  \item Whisper struggles to maintain alignment when transcribing long form data\cite{whisper}, so the approximate 10-minute length of each recording requires shortening to maintain system performance;
  \item TextGrids are not human-readable; and
  \item The output of the ASR system should be stored alongside the reference transcripts to ease evaluation, which is not possible using TextGrids.
\end{enumerate}

The solution to the first problem would best be solved by splitting the conversation recordings into individual utterances.
Luckily, the human-evaluated references include metadata which shows the start- and stop-times of each word and non-word part of speech, meaning it can easily be split into individual utterances without using voice activation detection or other automatic techniques.
Using the documentation for LifeLUCID it was possible to determine that there are two types of non-speech token;

\begin{enumerate}
  \item 'Break' tokens -- these are tokens which denote the speaker is not mid-utterance; either listening to the other participant or engaged in irrelevant discussion (these latter parts are silenced in the recordings).
  \item 'Junk' tokens -- these denote either:
    \begin{itemize}
      \item The speaker has paused (but the other participant is not speaking).
      \item A bell or dog bark is being played as part of their task (these are silent in the recording).
      \item Hesitations (e.g., 'umm', 'uhhh', etc.)
      \item Other non-speech, non-breaking tokens (not specified in their documentation but present in the transcripts).
    \end{itemize}
\end{enumerate}

For the purpose of this work, an utterance is defined as an uninterrupted piece of speech without any long pauses.
By providing threshold values for the minimum length of a 'break' token and maximum length of a 'junk' token, the boundaries at which utterances start and end can be easily computed from the reference TextGrids.
The utterance boundaries can then be used to extract individual utterances from the full-length recordings, resulting in a series of numbered audio files.

The second and third problems can be solved together by changing from the TextGrid format to JSON (JavaScript Object Notation).
This format is human-readable\cite{nurseitov2009comparison} and able to hold all the data and metadata required for this work, including a way to reference the original piece of audio it represents.

\section{Running Whisper}

\mycomment{
  Would it be worth moving some of this to chap3? defining a 'target user' is probably better suited to there...
  }
Whisper comes packaged with models of various sizes, requiring between approximately 1 and 10GB of VRAM and an increasing amount of time to produce transcripts.
Considering the objective to create an automatic transcription system which uses entirely free and open-source software, it is worth assuming that the individuals or institutions who would benefit the most from this system are those without the resources to rely on professional manual transcription.
It follows, then, that these 'target users' would not have access to high-powered computers and instead rely on consumer-grade hardware to generate transcriptions.
Thus, for the purposes of this work, the medium English-language model was selected as it requires only 5GB of VRAM and takes approximately half as much time to produce transcripts as the larger models\cite{whisper}.

\subsection{High-Powered Computing}

\section{Confidence Scoring}

\mycomment{
  describe confidence:
    * some way of knowing how likely the system is to be correct
    * difficult for one-shot evaluation as there is no training / test sets, just data
}
Whisper does not have a clear confidence scoring system in its unaltered state.
Therefore, for it to be viably used to aid a human transcriber, some method to estimate system confidence must be implemented.
At a high level, there are two sources from which to estimate confidence: Whisper's standard output, and its internal processes.
Table \ref{table:confidence-motivations} gives a brief comparison of the benefits and drawbacks associated with each of these sources of confidence;

\mycomment{
  Perhaps get rid of this table? seems to be all screwed up... and serves little benefit
  }
\include{tables/confidence-motivations}

The following subsections provide a detailed explanation of how each approach works and should help illustrate the benefits and drawbacks of each.

\subsection{Confidence From Model Output}
\mycomment{
  * avg_logprob
  * temperature
  * compression_ratio
  * no_speech_prob
  * could make an MLP?
  * drawbacks outway benefit (benefit being lack of modification (ie easier))
    > no word-level scoring, just average for an utterance
    > how is lprob calculated? (leads to next part)
}
Though it does not yield a clear confidence score, Whisper does output various statistics relating to its processing of the input data.
These include;

\begin{itemize}
  \item \texttt{avg\_logprob} -- The average of the $\log$ token-probability for a segment of speech (discussed further below)
  \item \texttt{temperature} -- 
  \item \texttt{compression\_ratio}
  \item \texttt{no\_speech\_prob}
\end{itemize}

\subsection{Confidence From Model Internals}
\mycomment{
  * how does the decoder decide on an output?
    > scoring is based on encoder output and has weighting from language model
    > using both of these sources of scoring there should be *some* indication of confidence
  * logits
    > how do they work?
    > what could they show?
  * potential drawbacks of using internal scoring
    > scores are only within an utterance! not well compared to other utterances
    > does this matter?
  * would give scores for each token (word or even sub-word level)
}

\section{A System For Transcription}
\mycomment{
  I reckon this section shouldn't use screenshots yet, just give examples of coloured text, ordering and blanking.
  Save screenshots for Implementation chapter
  }
