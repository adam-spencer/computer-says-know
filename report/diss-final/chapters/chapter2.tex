\chapter{Literature Survey}\label{ch:literature-survey}
\mycomment{
    Include a bit of text here to explain the structure of this section.

    * remember to do some comparisons between papers! 
    * this section should look at trends in the area of my  research
    * fake-ness (or 'synthetic'-ness) is correct!
  }

\section{Automatic Speech Recognition}\label{sec:what-is-asr}

\subsection{What is ASR?}

Automatic Speech Recognition (ASR) is a technology which allows computers to recognise and produce a text transcription of spoken language.
The research and development of technology involving speech has been a part of computer science since the late 1930s\cite{Rabiner2004Jan,vocoder}, with rudimentary ASR systems being constructed as early as the 1950s\cite{asr-52}.
These early attempts at recognising human speech treated it as a `pattern matching' problem, the theory being that words could be constructed by matching the pattern created in a speech signal to corresponding spoken phonemes\cite{Rabiner2004Jan}. 
This paradigm falls apart when the system must be re-tuned for each individual, even for simple tasks such as recognising spoken digits\cite{asr-52}, due to the fact that individual speakers don't produce exactly the same signal for each phoneme\cite{Horton2010}.

\mycomment{
        how does HMM work?
                * treat speech as a continuous sequence of discrete states
                * each state is modelled as a feature vector corresponding to an acoustic signal
                * assume that each state is generated from a probabilistic distribution correlated with another state in the model
                * through training, the model may estimate which states best correlate with an input signal
                * the most likely sequence of states is decoded, generating a transcription

        problems with HMM?
                * assumption that successive frames of speech are independent, thus the probability of a sequence of observations may be written as a product of the probabilities of individual observations
                * assumption that distirbutions of individaul observation params can be well represented as a mixture of Gaussian or autoregressive densities
                * markov assumption - prob of being in a state at time 't' is only dependednt on the state at time 't-1' is innapropriate for speech because dependencies often extend through several states
        }

Since the 1970s, finding the solution to the problem of pattern matching for speech recognition has been considered unviable through the precise matching of patterns but instead finding the most probable pattern using statistical modelling\cite{Rabiner2004Jan}.
The method which became most widely adopted and is still at the heart of modern ASR is Hidden Markovian Modelling, which was first applied to ASR in the '70s\cite{baker1975stochastic} and continued through the '90s\cite{bengio1999markovian} until today\cite{hmm2023}.

\subsection{Hidden Markov Models}

In his 1960 work\cite{dynkin1960}, Dynkin describes a Markov process using the example of a randomly-moving particle in space;

\say{
        If the position of the particle is known at the instant $t$, supplementary information regarding the phenomena observed up till the instant $t$ (and in particular, regarding the nature of the motion until $t$) has no effect on prognosis of the motion after the instant $t$ (for a known "present", the "future" and the "past" are independent of eachother).
}

From his description, we can draw the following assumptions for modelling a system as a Markov process;

\begin{itemize}
  \item The system consists of states.
  \item The system is \emph{in motion}, i.e. moving between states.
  \item This motion is random.
  \item The motion observed prior to $t$ (say, $t-1$) does not influence $t+k$ where $k \geq 1$.
  \item Because the particle is constantly moving between states, the state at time $t$ depends only on the state immediately prior, $t-1$.
  \item There is some probability, $p$, that the system moves from one state to another.
\end{itemize}

In a \emph{Hidden} Markov Model (HMM), the states and transition probabilities between them are known, but for some ouput sequence the order and selection of states used to produce the output is not known.
Knowing both the states and transition probabilities, it is therefore possible to calculate the most probable set of inputs used to produce the output.

To apply this model to speech, treat speech as a continuous sequence of discrete states, where each state is a feature vector representing an acoustic signal (either whole words, phonemes or even sub-phonetic features\cite{bengio1999markovian}).
Assuming that each state is generated from a probabilistic distribution correlated with other states in the model\cite{Rabiner1989Feb} (i.e. probability that one state follows another) and having trained these distributions on known data, the output signal (i.e. the speech signal) can be used to determine the most probable sequence of tokens spoken.
These tokens may then be decoded by a language model to construct a transcription\cite{bengio1999markovian}.

Despite making up much of the research foundational to modern ASR, Markov models have a crucial flaw when applied to speech; parts of speech are dependent on more than just the part immediately before (i.e. $t-1$).
For instance in a presentation discussing \emph{hats} it is unlikely that the word \emph{cat} would be used, despite the phonetics of the word being largely the same.

\subsection{How Does Modern ASR Work?}
\mycomment{
        'attention is all you need' - encoder-decoder transformer

        neural networks and transformers!
  }
Nowadays ASR has moved towards what is known as the 'end-to-end' or 'encoder-decoder' model;
at a high level, the input audio is encoded into features, these features are aligned with language and then decoded to produce an output transcript\cite{wang2019overview}.
The key difference between modern approaches and the classical HMM-based approach is the use of widely researched 'machine-learning' techniques, including various forms of neural network\cite{mustafa2019comparative, amodei2016deep, hori2017advances, Kim2017Mar}.

Recent research has proposed a new network architecture called the \emph{Transformer}\cite{vaswani2017attention}, aiming to reduce the computational complexity of encoder-decoder models by forgoing convolution or recurrent neural networks (RNNs) and instead relying on 'self-attention'.
The motivation for the \emph{Transformer} can be understood as follows;

\begin{itemize}
\item Convolution 
\end{itemize}

\subsection{Problems in ASR}

Despite their ubiquity, modern ASR systems aren't without fault.
Cutting edge systems like \emph(wav2vec) are capable of achieving greater-than-human scores on specific datasets\cite{wav2vec2,bigssl,chung2021} such as \emph{LibriSpeech}\cite{librispeech}, achieving as low as 1.4\% error\cite{zhang2020}.

A major problem with comes when the data is not `clean', for example, background noise is present, microphones are far away, the speaker has an atypical speech pattern, etc. 
In this setting, \emph{wav2vec} achieves much poorer scores with word error rates as high as 65\%\cite{whisper} on the \emph{CHiME6} corpus\cite{chime6}.

\mycomment{
        do more on problems here. set the scene for why transcription can't just be performed entirely by a computer (to be expanded on in the section about semi-/full-auto transcription)
    }

\subsection{Whisper}

In late September 2022, the OpenAI research laboratory (known for such projects as GPT-3/4 and ChatGPT) released a new open-source ASR system known as `Whisper'~\cite{whisper}.
Whisper is unique in being very large (trained on 680,000 hours of speech data), open-source, and fully supervised;
all the training data used to create the model has been accurately labeled and quality-checked by humans, unlike much larger unsupervised (or semi-supervised) models such as `BigSSL' (1,000,000+ hours of data)~\cite{bigssl}.

Unsupervised training is appealing for training speech recognisers because there is a wealth of unlabeled recordings, and labeled recordings are uncommon for less widely-spoken languages\cite{baevski2021}.
Unsupervised systems have a clear disadvantage, however, when compared to supervised; they lack clear decoder mappings\cite{whisper}, meaning that even for a succesfully encoded input there may not be a clear mapping from that input into a speech token.
To solve this, fine-tuning to map encodings to decodings is done on the part of the model's developers, though this is a precarious route to overfitting;
if the model is too fine-tuned to its training data, performance will suffer when faced with data which isn't well represented in the training set.

For example, if an unsupervised model were trained using the voices of young people, it may perform with considerably poorer accuracy when used to transcribe elderly speakers due to differences inherent to their speech\cite{Horton2010}.

Whisper uses a natural language model to perform next-token prediction (in layperson's terms, there is a secondary system trying to ensure the intelligibility of sentences produced from transcription).
In a practical setting this means that conversational speech (i.e.\ speech which flows as sentences rather than semantically-disjoint terms) should be transcribed with a higher degree of accuracy.

\subsection{ASR Confidence}
\mycomment{
        This could probably be a section of its own- there's lots to talk about here!
    }

\section{Speech Corpora}\label{sec:}
\mycomment{
  I think this section should be short and sweet. Just lay out what a corpus is, the history of corpora, conversational corpora, 
}

\section{Understanding Transcription}\label{sec:transcription}
\mycomment{
    Perhaps move this down a bit?
  }

\subsection{Manual Transcription}\label{subsec:manual-transcription}

\subsection{Semi-Automatic Transcription}\label{subsec:semi-auto-transcription}

\subsection{Fully-Automatic Transcription}\label{subsec:full-auto-transcription}

\section{Summary}\label{sec:lit-survey-summary}
\mycomment{
        is this strictly necessary? maybe look at what others have done in their dissertations.

        intuitively I think that the next chapter should build on what I've said here
    }
