\chapter{Literature Survey}\label{ch:literature-survey}
\mycomment{
    Include a bit of text here to explain the structure of this section.

    * remember to do some comparisons between papers! 
    * this section should look at trends in the area of my  research
    * fake-ness (or 'synthetic'-ness) is correct!
  }

\section{Automatic Speech Recognition}\label{sec:what-is-asr}

\subsection{What is ASR?}

Automatic Speech Recognition (ASR) is a technology which allows computers to recognise and produce a text transcription of spoken language.
The research and development of technology involving speech has been a part of computer science since the late 1930s\cite{Rabiner2004Jan,vocoder}, with rudimentary ASR systems being constructed as early as the 1950s\cite{asr-52}.
These early attempts at recognising human speech treated it as a `pattern matching' problem, the theory being that words could be constructed by matching the pattern created in a speech signal to corresponding spoken phonemes\cite{Rabiner2004Jan}. 
This paradigm falls apart when the system must be re-tuned for each individual, even for simple tasks such as recognising spoken digits\cite{asr-52}, due to the fact that individual speakers don't produce exactly the same signal for each phoneme\cite{Horton2010}.

\mycomment{
        how does HMM work?
                * treat speech as a continuous sequence of discrete states
                * each state is modelled as a feature vector corresponding to an acoustic signal
                * assume that each state is generated from a probabilistic distribution correlated with another state in the model
                * through training, the model may estimate which states best correlate with an input signal
                * the most likely sequence of states is decoded, generating a transcription

        problems with HMM?
                * assumption that successive frames of speech are independent, thus the probability of a sequence of observations may be written as a product of the probabilities of individual observations
                * assumption that distirbutions of individaul observation params can be well represented as a mixture of Gaussian or autoregressive densities
                * markov assumption - prob of being in a state at time 't' is only dependednt on the state at time 't-1' is innapropriate for speech because dependencies often extend through several states
        }

Since the 1970s, finding the solution to the problem of pattern matching for speech recognition has been considered unviable through the precise matching of patterns but instead finding the most probable pattern using statistical modelling\cite{Rabiner2004Jan}.
The method which became most widely adopted and is still at the heart of modern ASR is Hidden Markovian Modelling, which was first applied to ASR in the '70s\cite{baker1975stochastic} and continued through the '90s\cite{bengio1999markovian} until today\cite{hmm2023}.

\subsection{Hidden Markov Models}

In his 1960 work\cite{dynkin1960}, Dynkin describes a Markov process using the example of a randomly-moving particle in space;

\say{
        If the position of the particle is known at the instant $t$, supplementary information regarding the phenomena observed up till the instant $t$ (and in particular, regarding the nature of the motion until $t$) has no effect on prognosis of the motion after the instant $t$ (for a known "present", the "future" and the "past" are independent of eachother).
}

From his description, we can draw the following assumptions for modelling a system as a Markov process;

\begin{itemize}
  \item The system consists of states.
  \item The system is \emph{in motion}, i.e. moving between states.
  \item This motion is random.
  \item The motion observed prior to $t$ (say, $t-1$) does not influence $t+k$ where $k \geq 1$.
  \item Because the particle is constantly moving between states, the state at time $t$ depends only on the state immediately prior, $t-1$.
  \item There is some probability, $p$, that the system moves from one state to another.
\end{itemize}

In a \emph{Hidden} Markov Model (HMM), the states and transition probabilities between them are known, but for some ouput sequence the order and selection of states used to produce the output is not known.
Knowing both the states and transition probabilities, it is therefore possible to calculate the most probable set of inputs used to produce the output.

To apply this model to speech, treat speech as a continuous sequence of discrete states, where each state is a feature vector representing an acoustic signal (either whole words, phonemes or even sub-phonetic features\cite{bengio1999markovian}).
Assuming that each state is generated from a probabilistic distribution correlated with other states in the model\cite{Rabiner1989Feb} (i.e. probability that one state follows another) and having trained these distributions on known data, the output signal (i.e. the speech signal) can be used to determine the most probable sequence of tokens spoken.
Using a language model, these tokens may be decoded to construct a transcription\cite{bengio1999markovian}.

Despite making up much of the research foundational to modern ASR, Markov models have a crucial flaw when applied to speech; parts of speech are dependent on more than just the part immediately before (i.e. $t-1$).
For instance in a presentation discussing \emph{hats} it is unlikely that the word \emph{cat} would be used, despite the phonetics of the word being largely the same.

\mycomment{
        might be worth giving a bit more of an understanding of how much the field has 'blown up' since its inception.

        * what came before CNNs?
        * why are CNNs used now but not pre-y2k?
    }

with todays cutting-edge systems using Convolutional Neural Networks (CNNs)\cite{whisper,wav2vec2,bigssl,chung2021}, a method which requires massive quantities of data and computing power.

Today, speech recognition systems are ubiquitous in everyday computing tasks; 
integrated into operating systems and search engines, with uses ranging from `virtual assistants' (e.g., Apple's Siri, Amazon's Alexa) to providing people with disabilities the means to operate computer systems.

\subsection{How Does Modern ASR Work?}
\mycomment{
    This section should adress the technical ins and outs of how ASR works, featuring a diagram or two.

    This'd be a nice way to demonstrate my understanding of the workings of an ASR system.

    Also mention supervised and unsupervised systems and the advantages/disadvantages of each.
  }

\subsection{Problems in ASR}

Despite their ubiquity, modern ASR systems aren't without fault.
Cutting edge systems like \emph(wav2vec) are capable of achieving greater-than-human scores on specific datasets\cite{wav2vec2,bigssl,chung2021} such as \emph{LibriSpeech}\cite{librispeech}, achieving as low as 1.4\% error\cite{zhang2020}.

A major problem with comes when the data is not `clean', for example, background noise is present, microphones are far away, the speaker has an atypical speech pattern, etc. 
In this setting, \emph{wav2vec} achieves much poorer scores with word error rates as high as 65\%\cite{whisper} on the \emph{CHiME6} corpus\cite{chime6}.

\mycomment{
        do more on problems here. set the scene for why transcription can't just be performed entirely by a computer (to be expanded on in the section about semi-/full-auto transcription)
    }

\subsection{Whisper}

In late September 2022, the OpenAI research laboratory (known for such projects as GPT-3/4 and ChatGPT) released a new open-source ASR system known as `Whisper'~\cite{whisper}.
Whisper is unique in being very large (trained on 680,000 hours of speech data), open-source, and fully supervised;
all the training data used to create the model has been accurately labeled and
quality-checked by humans, unlike the much larger unsupervised `BigSSL' model (1,000,000+ hours of data)~\cite{bigssl}.

Whisper uses a natural language model to perform next-token prediction (in layperson's terms, there is a secondary system trying to ensure the intelligibility of sentences produced from transcription).
In a practical setting this means that conversational speech (i.e.\ speech which flows as sentences rather than semantically-disjoint terms) should be transcribed with a higher degree of accuracy.

\subsection{ASR Confidence}
\mycomment{
        This could probably be a section of its own- there's lots to talk about here!
    }

\section{Speech Corpora}\label{sec:}
\mycomment{
  I think this section should be short and sweet. Just lay out what a corpus is, the history of corpora, conversational corpora, 
}

\section{Understanding Transcription}\label{sec:transcription}
\mycomment{
    Perhaps move this down a bit?
  }

\subsection{Manual Transcription}\label{subsec:manual-transcription}

\subsection{Semi-Automatic Transcription}\label{subsec:semi-auto-transcription}

\subsection{Fully-Automatic Transcription}\label{subsec:full-auto-transcription}

\section{Summary}\label{sec:lit-survey-summary}
\mycomment{
        is this strictly necessary? maybe look at what others have done in their dissertations.

        intuitively I think that the next chapter should build on what I've said here
    }
