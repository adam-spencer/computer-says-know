\chapter{Literature Survey}\label{ch:literature-survey}
\mycomment{
    Include a bit of text here to explain the structure of this section.

    * remember to do some comparisons between papers! 
    * this section should look at trends in the area of my  research
    * fake-ness (or 'synthetic'-ness) is correct!
  }

\section{Automatic Speech Recognition}\label{sec:what-is-asr}

\subsection{What is ASR?}

Automatic Speech Recognition (ASR) is a technology which allows computers to recognise and produce a text transcription of spoken language.
The research and development of technology involving speech has been a part of computer science since the late 1930s\cite{Rabiner2004Jan,vocoder}, with rudimentary ASR systems being constructed as early as the 1950s\cite{asr-52}.
These early attempts at recognising human speech treated it as a `pattern matching' problem, the theory being that words could be constructed by matching the pattern created in a speech signal to corresponding spoken phonemes\cite{Rabiner2004Jan}. 
This paradigm falls apart when the system must be re-tuned for each individual, even for simple tasks such as recognising spoken digits\cite{asr-52}, due to the fact that individual speakers don't produce exactly the same signal for each phoneme\cite{Horton2010}.

\mycomment{
        might be worth giving a bit more of an understanding of how much the field has 'blown up' since its inception.

        * what came before CNNs?
        * why are CNNs used now but not pre-y2k?
    }

In the 1970s, statistical methods including Hidden Markov Modelling (HMM) became a clear pathway to improving speech recognition.
By viewing
\mycomment{
        how does HMM work?
                * treat speech as a continuous sequence of discrete states
                * each state is modelled as a feature vector corresponding to an acoustic signal
                * assume that each state is generated from a probabilistic distribution correlated with another state in the model
                * through training, the model may estimate which states best correlate with an input signal
                * the most likely sequence of states is decoded, generating a transcription

        problems with HMM?
                * assumption that successive frames of speech are independent, thus the probability of a sequence of observations may be written as a product of the probabilities of individual observations
                * assumption that distirbutions of individaul observation params can be well represented as a mixture of Gaussian or autoregressive densities
                * markov assumption - prob of being in a state at time 't' is only dependednt on the state at time 't-1' is innapropriate for speech because dependencies often extend through several states
        }

Since the 1970s the problem of speech recognition has been viewed more as one to be solved using statistical methods than by attempting to precisely match patterns\cite{Rabiner2004Jan}.


Such methods are still incorporated into modern research, such as Hidden Markovian Models, which were first applied to ASR in the '70s\cite{baker1975stochastic} but continue to play a part in new research\cite{hmm2023,bengio1999markovian}.


with todays cutting-edge systems using Convolutional Neural Networks (CNNs)\cite{whisper,wav2vec2,bigssl,chung2021}, a method which requires massive quantities of data and computing power.

Today, speech recognition systems are ubiquitous in everyday computing tasks; 
integrated into operating systems and search engines, with uses ranging from `virtual assistants' (e.g., Apple's Siri, Amazon's Alexa) to providing people with disabilities the means to operate computer systems.

\subsection{How Does Modern ASR Work?}
\mycomment{
    This section should adress the technical ins and outs of how ASR works, featuring a diagram or two.

    This'd be a nice way to demonstrate my understanding of the workings of an ASR system.

    Also mention supervised and unsupervised systems and the advantages/disadvantages of each.
  }

\subsection{Problems in ASR}

Despite their ubiquity, modern ASR systems aren't without fault.
Cutting edge systems like \emph(wav2vec) are capable of achieving greater-than-human scores on specific datasets\cite{wav2vec2,bigssl,chung2021} such as \emph{LibriSpeech}\cite{librispeech}, achieving as low as 1.4\% error\cite{zhang2020}.

A major problem with comes when the data is not `clean', for example, background noise is present, microphones are far away, the speaker has an atypical speech pattern, etc. 
In this setting, \emph{wav2vec} achieves much poorer scores with word error rates as high as 65\%\cite{whisper} on the \emph{CHiME6} corpus\cite{chime6}.

\mycomment{
        do more on problems here. set the scene for why transcription can't just be performed entirely by a computer (to be expanded on in the section about semi-/full-auto transcription)
    }

\subsection{Whisper}

In late September 2022, the OpenAI research laboratory (known for such projects as GPT-3/4 and ChatGPT) released a new open-source ASR system known as `Whisper'~\cite{whisper}.
Whisper is unique in being very large (trained on 680,000 hours of speech data), open-source, and fully supervised;
all the training data used to create the model has been accurately labeled and
quality-checked by humans, unlike the much larger unsupervised `BigSSL' model (1,000,000+ hours of data)~\cite{bigssl}.

Whisper uses a natural language model to perform next-token prediction (in layperson's terms, there is a secondary system trying to ensure the intelligibility of sentences produced from transcription).
In a practical setting this means that conversational speech (i.e.\ speech which flows as sentences rather than semantically-disjoint terms) should be transcribed with a higher degree of accuracy.

\subsection{ASR Confidence}
\mycomment{
        This could probably be a section of its own- there's lots to talk about here!
    }

\section{Speech Corpora}\label{sec:}
\mycomment{
  I think this section should be short and sweet. Just lay out what a corpus is, the history of corpora, conversational corpora, 
}

\section{Understanding Transcription}\label{sec:transcription}
\mycomment{
    Perhaps move this down a bit?
  }

\subsection{Manual Transcription}\label{subsec:manual-transcription}

\subsection{Semi-Automatic Transcription}\label{subsec:semi-auto-transcription}

\subsection{Fully-Automatic Transcription}\label{subsec:full-auto-transcription}

\section{Summary}\label{sec:lit-survey-summary}
\mycomment{
        is this strictly necessary? maybe look at what others have done in their dissertations.

        intuitively I think that the next chapter should build on what I've said here
    }
