\chapter{Literature Survey}\label{ch:literature-survey}
\mycomment{
    Include a bit of text here to explain the structure of this section.

    * remember to do some comparisons between papers! 
    * this section should look at trends in the area of my  research
    * fake-ness (or 'synthetic'-ness) is correct!

    Try and stick some figures in to spice it up a bit, currently reads a tad bare
}
The purpose of this chapter is to develop an understanding of the existing literature on the topics of automatic speech recognition, neural network confidence, speech corpora, and computer-aided transcription.
Each topic is discussed in its own section, with subsections to explore parts of each topic in greater depth.

\section{Automatic Speech Recognition}\label{sec:what-is-asr}

\subsection{What is ASR?}

Automatic Speech Recognition (ASR) is a term used to describe technology which allows computers to recognise and produce a text transcription of spoken language.
The research and development of technology involving speech has been a part of computer science since the late 1930s\cite{Rabiner2004Jan,vocoder}, with rudimentary ASR systems being constructed as early as the 1950s\cite{asr-52}.
These early attempts at recognising human speech treated it as a `pattern matching' problem, the theory being that words could be constructed by matching the pattern created in a speech signal to corresponding spoken phonemes\cite{Rabiner2004Jan}.
Despite speech recognition fundamentally being a problem of matching speech patterns to text, these early attempts were not particularly robust; requiring re-tuning by a human operator in order to match the specificities of each new speaker's speech patterns\cite{asr-52}.

The 1970s saw the application of statistical techniques to improve the robustness of ASR systems\cite{Rabiner2004Jan}, the most widely adopted method being the application of 'hidden Markov models' (HMMs)\cite{baker1975stochastic}.
Use of HMMs continued through the '90s\cite{bengio1999markovian} and is still in use today\cite{hmm2023}.

\subsection{Hidden Markov Models}
\mycomment{
        Motivate the exploration of HMMs -> to show progress in the field and understand which problems persist
}
In order to better understand the way in which modern ASR systems have developed, why they function the way they do, and what limitations have yet to be solved, it's important to explore the approach which historically saw the widest adoption; hidden Markov models.

First, what is a 'Markov model' (also known as a Markov process)?
In his 1960 work\cite{dynkin1960}, Dynkin describes a Markov process using the example of a randomly-moving particle in space;

\say{
        If the position of the particle is known at the instant $t$, supplementary information regarding the phenomena observed up till the instant $t$ (and in particular, regarding the nature of the motion until $t$) has no effect on prognosis of the motion after the instant $t$ (for a known "present", the "future" and the "past" are independent of each other).
}

From his description, we can draw the following assumptions for modelling a system as a Markov process;

\begin{itemize}
  \item The system consists of states.
  \item The system is \emph{in motion}, i.e. moving between states.
  \item This motion is random.
  \item The motion observed prior to $t$ (e.g. $t-1$) does not influence the motion following $t+k$ where $k \geq 1$.
  \item Because the particle is constantly moving between states, the state at time $t$ depends only on the state immediately prior, $t-1$.
  \item There is some probability, $p$, that the system moves from one state to another.
\end{itemize}

In a \emph{Hidden} Markov Model (HMM), the states and transition probabilities between them are known, but for some output sequence the order and selection of states used to produce the output is not known.
Knowing both the states and transition probabilities, it is therefore possible to calculate the most probable set of inputs used to produce the output.

To apply this model to speech, treat speech as a continuous sequence of discrete states, where each state is a feature vector representing an acoustic signal (either whole words, phonemes or even sub-phonetic features\cite{bengio1999markovian}).
Assuming that each state is generated from a probabilistic distribution correlated with other states in the model\cite{Rabiner1989Feb} (i.e. probability that one state follows another) and having trained these distributions on known data, the output signal (i.e. the speech signal) can be used to determine the most probable sequence of tokens spoken.
These tokens may then be decoded by a language model to construct a transcription\cite{bengio1999markovian}.

Despite making up much of the research foundational to modern ASR, Markov models have a crucial flaw when applied to speech; parts of speech are dependent on more than just the part immediately before (i.e. $t-1$).
For instance, in a presentation discussing \emph{hats} it is unlikely that the word \emph{cat} would be used, despite the phonetics of the word being largely the same.

\subsection{The Transformer}\label{subsec:modern-asr}
\mycomment{
  Maybe start off with just looking at transformers- encoder/decoder and HMM aren't mutually exclusive!
}

Skipping ahead from the mid-1980s to the current day 'state-of-the-art', ASR has moved towards what is known as the 'encoder-decoder' model.
At a high level, this model consists of two key parts; an encoder and a decoder.
The encoder processes (\emph{encodes}) input audio into features, these features are aligned with language and then processed by the decoder (\emph{decoded}) to produce an output transcript\cite{wang2019overview}.
The key difference between modern approaches and the classical HMM-based approach is the use of widely researched 'machine learning' techniques, including various forms of neural network\cite{mustafa2019comparative, amodei2016deep, hori2017advances, Kim2017Mar}.

Recent research has proposed a new network architecture called the \emph{Transformer}\cite{vaswani2017attention}, aiming to reduce the computational complexity of encoder-decoder models by forgoing convolutional or recurrent neural networks (CNNs and RNNs) and instead relying on 'attention'.
The motivation for the \emph{Transformer} can be understood as follows;

\begin{itemize}
        \item CNNs (e.g. \cite{zeghidour2018fully}) and RNNs (e.g. \cite{graves2014towards}), while popular, have greater per-layer computational complexity than self-attention\cite{vaswani2017attention}.
        \item Recurrent neural networks must perform $O(n)$ sequential operations for a sequence length $n$, whereas self-attention has a constant (i.e. $O(1)$) maximum number of sequential operations, enabling parallel computation\cite{vaswani2017attention}.
        \item By allowing each layer in the encoder and decoder to attend to the whole output of the previous layer, 
\end{itemize}

In a multilayer network, attention layers are used to build relations between separate parts of an input sequence by allowing each node (or 'attention head'\cite{shaw2018self}) to attend to all outputs from the previous layer.
A technique refered to as 'multi-head attention'\cite{vaswani2017attention} enables attention to be calculated in parallel for all inputs in a sequence.\\

The calculation for attention is:

\[ \text{Attention}(Q,K,V) = \softmax (\frac{QK^T}{\sqrt{d_k}})V \]

Where

\begin{itemize}
        \item $Q$ is known as the \emph{Query} vector,
        \item $K$ is known as the \emph{Key} vector, and
        \item $V$ is known as the \emph{Value} vector.
\end{itemize}

The output of the attention function is described as \say{a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key}\cite{vaswani2017attention}.
To understand what these vectors denote in the context of speech recognition, we must understand what the different types of attention \emph{are} and how their use \emph{differs} in the encoder and decoder;

\begin{itemize}
        \item self-attention layers (in both the encoder and decoder) take input from the previous layer within the same block (i.e. encoder or decoder).
        In this layer, $Q$, $K$, and $V$ are all calculated by multiplying an input embedding (a vector representation of the raw input sequence) by three different trained weight vectors.
        \item cross-attention layers (present only in the decoder) enable mixing of the encoder's output with the output of the decoder's previous self-attention block, allowing the decoder to attend over the whole encoded input.
        In this layer, only $K$ and $V$ are taken from the encoder; $Q$ is dervied from the previous decoder block.
\end{itemize}

For the sake of clarity, a simplified version of the operation of the transformer (as explained in \cite{vaswani2017attention}) is as follows;

\begin{enumerate}
        \item An input sequence is transformed into an embedding, which is then input into the \emph{encoder stack} (the layers which make up the encoder) after being modified using \emph{positional embeddings}, which act to preserve information about the position of each part of the input sequence.
        \item The \emph{attention head} of each self-attention layer in the encoder first multiplies the input by three trained weight matrices to acquire the values of $Q$, $K$, and $V$, then calculates Attention using these values as input.
        Each attention head is trained with its own weight matrix, so there are an equal number of Attention results and attention heads.
        The resulting attention matrices are concatenated and then multiplied by another weight matrix to produce the layer's output to be fed forward to the next layer.
        \item The output of the final encoder layer is transformed into a pair of $K$ and $V$ vectors and used as input (via \emph{cross-attention} for the decoder, which works almost identically to the encoder (i.e. a stack of self-attention blocks).
        The key difference between the encoder and decoder is that the decoder uses previous decoded output as its $Q$ vector and it may not self-attend to all of the output at once, only the output earlier to its current position (unlike the encoder which attends to the whole sequence simultaneously)
        \item Finally, the output of the decoder goes through some normalisation steps to find and output the most probable \emph{output embedding}.
\end{enumerate}

An \emph{output embedding} may be thought of as a way to link together an expected output (e.g. a word) and a vector in one-hot encoding space.
In the Transformer's decoder this allows a score represented in a one-hot encoding to be matched to an output.

Though this is all rather complex, the key points to understand are that a Transformer uses both an encoder and decoder, the decoder can attend to only previously decoded output, and these two 'blocks' work together to produce an output.

\subsection{Evaluating ASR Systems}
\mycomment{
        WER is most used (back this statement up)
        Caveats for using WER
}
As with any computational model, the performance of an ASR system is evaluated in terms of both speed and accuracy.
Calculating the speed of a model is simple enough; just time it!
Accuracy, however, is not so simple because there are multiple metrics upon which a reference and ASR-generated transcript (known as a 'hypothesis') may differ.

There are three different levels at which to calculate the similarity between a reference and hypothesis; the word-level, phoneme-level, and character-level\cite{fang2020}
Comparing words and characters is relatively self-explanatory, the transcripts are either split up into individual words or characters for comparison.
A phoneme-level comparison involves transforming textual words into the parts of speech which constitute them, known as phonemes.

The accuracy calculation to derive an 'error-rate' (also known as the edit-distance\cite{niessen2000evaluation}) is as follows;

\[WER = \frac{S + D + I}{S + D + C}\]

where:

\begin{itemize}
  \item $S$ is the number of \emph{substituted} words (words which appear in the place of another correct word)
  \item $D$ is the number of \emph{deleted} words (words which aren't present in the hypothesis but appear in the reference)
  \item $I$ is the number of \emph{inserted} words (words which do not appear in the reference but appear in the hypothesis)
  \item $C$ is the number of \emph{correct} words (words which appear in both the reference and hypothesis)
\end{itemize}

Word error-rate (WER) is the most commonly used metric in the literature\cite{park2008empirical}, though arguments can be made that it is not entirely representative of the degree to which a system has an understanding of speech.
Take, for example, a reference which contains a compound word like 'soundproof' or 'eggshell' -- if an ASR system were to output 'sound proof' or 'egg shell' it would have incurred one substitution and one deletion equating to a WER of 200\% despite having produced an output which is representative of the input.

Judging by calls to work to produce a metric to replace WER still being made today\cite{pasandi2022evaluation, szymanski2020we} and the relative lack of published ASR papers discussing results using metrics other than WER, there is not yet a viable alternative.

\subsection{Problems in ASR}

Despite their ubiquity, modern ASR systems aren't without fault.

Cutting edge systems like \emph(wav2vec) are touted as being capable of achieving 'greater-than-human' scores on specific datasets\cite{wav2vec2,bigssl,chung2021} such as \emph{LibriSpeech}\cite{librispeech}, achieving as low as 1.4\% error\cite{zhang2020}.
LibriSpeech consists entirely of English audiobook recordings which have been selected in-part based on their quality\cite{librispeech}; not particularly representative of everyday speech\cite{szymanski2020we}, lacking features such as speaker overlap which are common in conversation\cite{shriberg2001observations}.
An evaluation of modern proclaimed 'state-of-the-art' ASR systems found WER scores averaging approximately 17\% when faced with real-world conversational data\cite{szymanski2020we} despite reporting results below 4\% on LibriSpeech.

Racial disparities in the accuracy of ASR systems has also been reported, with average WER scores for black speakers almost double that of white speakers\cite{koenecke2020racial}.
The authors of this article attribute this performance difference to the acoustic models used in the ASR systems rather than the language models, thus concluding that there is a lack of training data from black speakers.

These two problems serve indicate that, in order to improve accuracy, ASR systems should be trained on more diverse data in terms of the context of the speech and the demographic of speakers.

\section{Whisper}

In late September 2022, the OpenAI research laboratory (known for such projects as GPT-3/4 and ChatGPT) released a new open-source ASR system called `Whisper'~\cite{whisper} which uses the encoder-decoder Transformer architecture discussed in section \ref{subsec:modern-asr}.
It is described as a 'zero-shot' model, meaning it is expected to produce good results \emph{without} any dataset-specific fine tuning.
Whisper is unique in being very large (trained on 680,000 hours of speech data), open-source, and fully supervised;
all the training data used to create the model has been accurately labeled and quality-checked by humans, unlike much larger unsupervised (or semi-supervised) models such as `BigSSL' (1,000,000+ hours of data)~\cite{bigssl}.

Unsupervised training is appealing for training speech recognisers because there is a wealth of unlabeled recordings, and labeled recordings are uncommon for less widely-spoken languages\cite{baevski2021}.
Unsupervised systems have a clear disadvantage, however, when compared to supervised; they lack clear decoder mappings\cite{whisper}, meaning that even for a successfully encoded input there may not be a clear mapping from that input into a speech token.
To solve this, fine-tuning to map encodings to decoded text tokens is done on the part of the model's developers, though this is a precarious route to overfitting;
if the model is too fine-tuned to its training data, performance will suffer when faced with data which isn't well represented in the training set.
For example, if an unsupervised model were trained using the voices of young people, it may perform with considerably poorer accuracy when used to transcribe elderly speakers due to differences inherent to their speech\cite{Horton2010}.

The way Whisper functions is discussed in detail in section \ref{sec:confidence-scoring}.

\section{Confidence}\label{sec:litreview-confidence}
\mycomment{
       Write about what confidence is, then in subsections cover literature on the subject.
}
Neural networks will always produce an output for an accepted input, no matter how likely or unlikely the output is of being correct.
The network may even assign a very large probability to its given output; consider, for example, a classifier trained to predict the city in the UK which someone was born in given the co-ordinates of their current address.
The classifier simply assigns the closest city to their address and often makes a correct prediction.
However, if the input co-ordinates were somewhere in Iceland it may assign a high probability to their place of birth being in Inverness because it is much closer than any of the other UK cities, even though the true probability that this hypothetical Icelander was born in Inverness is quite low.

The point of this example is to illustrate that the probability assigned by a neural network is not always a good indication of correctness.
An estimation of a model's expected correctness is refered to as \emph{confidence}, i.e. how \emph{confident} is the model that its prediction is correct?



\section{Speech Corpora}\label{sec:}
\mycomment{
  I think this section should be short and sweet. Just lay out what a corpus is, quick overview of the history of corpora, conversational corpora, 
}

\section{Transcription}\label{sec:transcription}
\mycomment{
    Perhaps move this down a bit?
  }

\subsection{Manual Transcription}\label{subsec:manual-transcription}

\subsection{Fully-Automatic Transcription}\label{subsec:full-auto-transcription}

\subsection{Semi-Automatic Transcription}\label{subsec:semi-auto-transcription}

\section{Summary}\label{sec:lit-survey-summary}
\mycomment{
    }
