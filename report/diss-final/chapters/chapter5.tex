\chapter{Implementation}\label{ch:implementation-and-testing}

The aim of this chapter is to provide some more insight into the choices made while implementing the design as discussed in the previous chapter and explain the motivations that lead to those choices.

\section{Data Processing}

TextGrid data was broken into 'utterances' using a script called \texttt{get\_utterances} (available in appendix \ref{appendix:get-utterances}).
This script takes as input a directory containing TextGrid data, an output directory and threshold values for the minimum break length and maximum pause length allowed in an utterance.
The output is in JSON format, generating a file to which other data is added.

The second step is to use the script \texttt{segment\_audio} to split up audio files into individual utterances within directories given the title of the original conversation they were extracted from.
This code is available in appendix \ref{appendix:segment-audio}.
Upon completion, the output of this script is a directory with subdirectories containing hundreds of audio files.

Thirdly, the script \texttt{do\_whisper\_confidence} was run (available in appendix \ref{appendix:do-whisper}) using the University of Sheffield's Bessemer HPC cluster\cite{shef-hpc}.
GPU clusters on Bessemer have access to Nvidia V100 GPUs with 32GB of VRAM, enabling transcriptions to be generated with Whisper very quickly.
Instead of using the standard Whisper model, the \emph{whisper-timestamped}\cite{whisper-timestamped} library is used because it can calculate word-level confidence scores.

The fourth and fifth steps to process data were to run the scripts \texttt{normalise\_text} and \texttt{calculate\_wer} (available in appendices \ref{appendix:normalise-text} \& \ref{appendix:calculate-wer} respectively).
The first of these scripts runs a text normaliser which is packaged with Whisper\cite{whisper} to perform English text normalisation, then the second script calculates WER scores and finds the per-word confidence scores for each word in the output.
WER is calculated using the \emph{jiwer}\cite{jiwer} library.

\begin{figure}[p]
\centering
\begin{BVerbatim}
"0": {
  "start": 15.04,
  "end": 15.74,
  "transcript": "okay",
  "whisper": {
    "text": "okay",
    "segments": [
      {
        "id": 0,
        "seek": 0,
        "start": 0.0,
        "end": 0.4,
        "text": " OK.",
        "tokens": [
          50363,
          7477,
          13,
          50440
        ],
        "temperature": 0.0,
        "avg_logprob": -0.7832436561584473,
        "compression_ratio": 0.2727272727272727,
        "no_speech_prob": 0.02687731385231018,
        "confidence": 0.317,
        "words": [
          {
            "text": "OK.",
            "start": 0.0,
            "end": 0.4,
            "confidence": 0.317
          }
        ]
      }
    ],
    "language": "en"
  },
  "confidence_scoring": {
    "words": "okay",
    "confidence_scores": [
      0.317
    ],
    "utterance_confidence": 0.317
  },
  "wer": 0.0,
  "avg_logprob": -0.7832436561584473,
  "file_measures": {
    "wer": 0.11153358681875793
  }
},
\end{BVerbatim}
  \caption{Example JSON entry from output}
  \label{fig:json-output-example}
\end{figure}

\clearpage
An example entry from the JSON output is presented in figure \ref{fig:json-output-example}.
The decision was made to preserve all of the output data from Whisper, meaning each entry is quite long.
An entry can be identified based on its filename and the number which indexes the entry ("0" in this case), where the filename of the accompanying audio segment has the same identifier (\texttt{000.wav} in this case).
As this example is the first entry in an output file, it contains a field called \texttt{file\_measures} which tracks the overall WER for the entire file.

This does, however, lead to the issue of readability re-emerging.
One of the goals of switching to JSON from TextGrid format was to improve the readability of the raw data.
Despite being kept in neat utterances, the data is not easily read as it is largely metadata rather than just the text.
Changing this would require removing some of the metadata (e.g. removing this 'words' field from the 'whisper' field), though the need for data preservation was ultimately considered paramount and thus the data remained in this format.

\subsection{Audio Issue}

During the first few experiments processing data through the script pipeline it was discovered that some audio files were being transcribed very poorly.
It was assumed that the same channels were labeled 'A' and 'B' for all of the recordings, however around half of the recordings use the opposite channel to the other.
This is important because the reference transcript TextGrid files are all labeled 'A' and 'B', thus the wrong audio channel had been segmented.

This issue was quickly diagnosed by calculating file-averages of the \texttt{no\_speech\_prob} output by Whisper, where it was discovered that half of the recordings were mostly silent (i.e. the speaker was listening to the other and not talking).
The conversations that had used the wrong channel were all discovered and the audio was re-segmented using the other channel.
The final corpus used for evaluation did not contain any of these errors.

\section{Demo Software}

The demo software's front-end is built using \emph{Textual}\cite{textual}, a Python library for terminal-based graphical applications.
A terminal-based system was decided on to ensure compatibility across operating systems while using Python, as other techniques like \emph{PyQt}\cite{pyqt} do not display the same on all OSs.
Rich comes with various 'widgets' which enable quick and simple building of terminal applications, including the \texttt{DataTable} which was used for the demo software.
The code for the front-end is available in appendix \ref{appendix:asr-app}.

To gather results, highlight and blank text, play audio and order results, a back-end was developed consisting of a simple class for each row and a class to represent all of the data in the table.
The back-end code is available in appendix \ref{appendix:audio-data-link}.

\subsection{Caveats}

The demo software lacks the ability to be used as a real piece of computer-aided transcription software;
it can't take user input and has occasional audio glitches.

These problems prevent it from being used in any setting outside of simple demonstration and would require modification to both the front-end design and the method for playing audio.
Audio glitches are relatively uncommon and dissipate once the user plays a piece of audio multiple times, though if the system is intended to be used accurately the audio should be reliable and incapable of confusing a listener.
User input could be taken by, for example, adding a new column which takes keyboard input or allowing users to overwrite 'blanked out' sections of text.
