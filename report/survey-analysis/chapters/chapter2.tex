\chapter{Literature Survey}\label{ch:literature-survey}

\section{What is ASR?}\label{sec:what-is-asr?}

Automatic Speech Recognition (ASR) is a subfield of computer science which develops technology
allowing computers to understand human speech.
At the most basic level, ASR consists of simply running a recording of human speech through a
computer model and producing a transcript of said speech.
Nowadays most ASR is conducted using deep learning models, trained on large datasets of human
speech and using varying pre- and post-processing steps.\\

The metric upon which ASR systems are compared is their accuracy, often reported as a percentage
word error rate (WER) where lower WER equals higher per-word accuracy.
WER is calculated as
\[
    \text{WER} = \frac{S + D + I}{N}
\]
Where $S$,$D$, and $I$ are substitutions, deletions, and insertions respectively, and $N$ is the
total number of words in the reference transcript~\cite{gaikwad2010review}.
Such systems may also be compared across other metrics (for example, time complexity) but these
comparisons won't be covered in detail in this work because they aren't relevant factors on system
accuracy.

\section{Speech Corpora}\label{sec:speech-corpora}

The large datasets of human-transcribed speech are referred to as `speech corpora'.
Each corpus contains recordings of speakers, accurate transcriptions, and various metadata, such as
the speaker's sex, age, and various other data about their background.

Speech corpora are fantastic resources for evaluating and comparing ASR techniques because they
are already labelled correctly and many have been published for a long time, meaning some corpora
have already been used to evaluate both early and more recent models.

\section{Modern ASR}\label{sec:modern-asr}

\subsection{Whisper}\label{subsec:whisper}

In late September 2022, the OpenAI research laboratory (known for their `GPT-3' language model)
released a new open-source ASR system known as `Whisper'~\cite{whisper}.
Whisper is unique in being very large (trained on 680,000 hours of speech data), open-source, and
fully supervised;
meaning all the training data used to create the model has been accurately labeled and
quality-checked by humans, unlike the much larger unsupervised `BigSSL' model (1,000,000+ hours
of data)~\cite{bigssl}.

Whisper uses a natural language model to perform next-token prediction (in layperson's
terms, there is a secondary system trying to ensure the intelligibility of sentences produced
from transcription).
Practically, this means that conversational speech (i.e.\ speech which flows as sentences rather
than disjoint terms) should be transcribed with a higher degree of accuracy.

\\
Their accuracy results are promising across a range of different speech corpora, outperforming
previous state-of-the-art `wav2vec 2.0'~\cite{wav2vec}.
Interestingly, Whisper doesn't achieve higher performance than some other models on specific
corpora, for example on LibriSpeech test-other~\cite{librispeech} it is outperformed by
two models built atop wav2vec~\cite{zhang2020,chung2021}, though across a more diverse set
of speech corpora Whisper achieves much lower WER~\cite{whisper}.

\subsection{wav2vec 2.0}\label{subsec:wav2vec}

The wav2vec 2.0 model, as well as its successors~\cite{wav2vec, chung2021, zhang2020}, are
readily available online via HuggingFace~\cite{huggingfacetransformers}.
Unlike Whisper, wav2vec is available in both supervised and unsupervised versions, meaning there
is some space for evaluation of supervised versus unsupervised with elderly speakers.
There is also substantial documentation regarding fine-tuning wav2vec models, giving more
flexibility to the potential path of this project.

\subsection{Closed-source services}\label{subsec:closed-source-services}

As with any technology, there are closed-source offerings from companies like Amazon and Google,
namely \emph{Amazon Transcribe} and Google Cloud's \emph{Speech-to-Text}.
These are offered as paid-for services, utilising the massive computing power available to these
companies rather than requiring direct access to high-powered computers.
It is not readily apparent which ASR systems are being used in these services, and, due to the
high cost required to use them, they will not be considered in this work.

\section{Elderly Speech}\label{sec:elderly-speech}

As we age, our speech changes in a variety of ways, as described in research
performed by Horton et al.~\cite{Horton2010}.
Their research shows strong correlation between age and an increase in filled pause rate,
decrease in rate of speech, and a higher `type/token ratio'.

\begin{itemize}
    \item A filled pause is a hesitation between words, for example `um' or
    `ah'~\cite{Maclay1959Jan};
    \item the rate of speech is simply the number of words spoken in a given length of time;
    and
    \item type/token ratio (TTR) is a measure of vocabulary diversity, calculated as a ratio of
    the number of unique different words spoken (types) to the total number of words spoken
    (tokens)~\cite{richards_1987}.
    For example, if a sentence was 20 words long but only contained 15 different words, the TTR of
    that sentence would be $\frac{15}{20} = 0.75$.
\end{itemize}

Evidence suggests that ASR is more error-prone for elderly speakers (\>65 years of
age) than other age groups~\cite{picone1990,vote400}.
Speaker age is shown to have a significant impact on the frequency spectrum present in their
speech\cite{Taylor2020Mar}.
This deviation in frequency may be to blame for some amount of ASR transcription error, though
this is something to be explored later in this work.

\section{Data Collection}\label{sec:data-collection}

The key requirement for comparing these systems is a large dataset of speech recordings over a
wide range of ages, with labels denoting the age of the speaker and an accurate transcript to
compare ASR performance against.
There is a surprising lack of relevant resources to date, with most speech corpora which feature
labeled speaker age not containing a diverse age range, and very few corpora oriented towards
elderly speakers.

Unless the lack of data becomes too detrimental to the completion of this work, only English data
shall be sought out because most published ASR systems function only when transcribing English
or achieve much poorer accuracy scores when transcribing other languages.

The LifeLUCID corpus was collected with the express purpose of providing a wide range of
speaker ages~\cite{lifelucid}.
This resource has great potential in providing an insight into the age-variable performance of
ASR because it features 104 British English speakers between 8 and 85 years of age.

Likewise, the MATCH corpus~\cite{Georgila2010Sep} contains

It is of interest to observe any patterns in errors made by each system, especially if there are
any similarities in where they lose accuracy.
Once the areas of error are understood, comparing and contrasting audio spectra may allow for
insight into which features of speech create problems for ASR\@.

\section{Comparing Models}\label{sec:comparing-models}

WER comparisons are useful for comparing average model-to-model performance but lack the ability
to find trends in errors outside of words which are repeatedly mislabeled or labels which are
frequently misplaced.

Alternatively, phonemes may be used instead of words to generate the \emph{phone error rate}
(PER) and this may provide a more useful insight into phonetic trends in ASR performance.
Speech corpora do not often provide phoneme labels, though these labels may be generated using
software such as the Python NLTK package~\cite{nltk}.

PER is shown to often be lower than WER~\cite{fang2020}, suggesting that ASR systems have
difficulty transcribing similar phonetic structures (i.e.\ words that sound almost identical).
Due to modern ASR systems like Whisper using a natural-language model, if many rare and complex
words follow each other, false predictions may create a domino-effect and cause an incorrect
transcription to be picked over a more accurate one.

The major issue with using PER is that ASR systems such as Whisper opt to use pre-trained
tokenizers~\cite{huggingfacetransformers} to decode tokens produced by the model into text to
form a transcript, thus limiting the ability to collect phonetic information directly from the
model.

By collating word-labelling errors, it may be possible to derive phoneme errors by comparing
these false labels as phonemes generated by NLTK\@.
A pattern drawn from phonetic errors should provide more insight into causes of poor labelling.

\subsection{Bespoke Systems}\label{subsec:bespoke-systems}

While most ASR systems aim to transcribe all speech of a specific language, irrespective of
speaker demographic, there may be some utility in the creation of a bespoke system which opts
only to produce accurate transcriptions of elderly speakers.

Work produced by Baba et al.\ demonstrates an increase in ASR accuracy for elderly speakers when
adapting an existing ASR model to include elderly speaker data\cite{baba2004}.
Also, in a preliminary experiment conducted by Jang et al.\ they claim a similar
finding~\cite{vote400} when fine-tuning another existing ASR model.

It follows, then, that with a large enough collection of elderly speaker data it may be possible
to adapt an English-language ASR system using elderly speaker data to produce improved results.