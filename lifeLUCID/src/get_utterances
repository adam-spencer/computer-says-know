#!/usr/bin/env python
"""
get_utterances

Given a directory of Praat TextGrid files, this program segments them all
into utterances according to the documentation provided by with the LifeLUCID
corpus (V.Hazan et al.).

The utterances are written to JSON files in a given output directory.

Copyright Adam Spencer, 03/2023
"""

"""
TODO:
  Allow user-specified minimum time distance / time threshold.
  Sometimes a single utterance is split in to multiple due to a 
  SILP token, despite the speaker not stopping all mouth noise.

  This could be easily remedied by allowing, say, a half-second
  minimum distance between utterances. This would prevent a single 
  spoken section being split up unnecesarily.

  Perhaps a maximum threshold between words too! This would mean 
  any pauses for umm-ing or SIL tokens that continue for too long
  cause the current utterance to end and the next begin.
"""

__author__ = 'Adam Spencer'

import argparse
import json
import textgrid as tg
from pathlib import Path
from typing import Union

# As described in documentation
BREAK_TOKENS = {'SILP', '<GA>'} 
JUNK_TOKENS = {'SIL', '<BELL>'}

def find_utterances(grid:tg.TextGrid, /, normalise:bool) -> (
    dict[int, dict[str, Union[float, str]]]):
  """
  Find all utterances in a Praat TextGrid file, returning start and end times and the
  transcription as provided in the TextGrid.

  Utterance ends and beginnings are found using the `BREAK_TOKENS`, and any
  non-speaking tokens (as defined in `JUNK_TOKENS`) are removed.

  :param grid: The Praat TextGrid to segment into utterances.
  :param normalise: Enable lowercase text normalisation.
  :returns: dict of structure { segment_num -> { start_time, end_time, transcript } }
  """
  segments = dict()
  start_time = float()
  seg_words = list()
  seg_counter = 0
  seg_ongoing = False

  for interval in grid[0]:
    if interval.mark in BREAK_TOKENS:
      if seg_ongoing:
        segments[seg_counter] = {'start' : start_time, 'end' : interval.minTime,
                                 'transcript' : ' '.join(seg_words)}
        seg_words = []
        seg_counter += 1
        seg_ongoing = False
      continue
    elif interval.mark in JUNK_TOKENS:
      continue
    elif not seg_ongoing:
      start_time = interval.minTime
      seg_ongoing = True
    if normalise:
      interval.mark = interval.mark.lower()
    seg_words.append(interval.mark)
  return segments

def write_to_json(utterances:dict, output_file:Path) -> None:
  """
  Write utterances out to the given output directory in JSON format.

  :param utterances: Utterances dict 
  structure { segment_num -> { start_time, end_time, transcript } }
  :param output_file: Path to output file.
  """
  with open(output_file, 'w') as f:
    json.dump(utterances, f,  indent=2)

def main():
  # Parse Arguments
  parser = argparse.ArgumentParser()
  parser.add_argument('in_dir', help='Name of dir to find textgrids in')
  parser.add_argument('out_dir', help='Name of dir to write JSON files to')
  parser.add_argument('--no-normalise', '-n', action='store_true',
                      help='Disable case normalisation')
  args = parser.parse_args()
  normalise = not args.no_normalise

  # New Path objects @ specified paths
  in_dir_path = Path(args.in_dir)
  out_dir_path = Path(args.out_dir)
  for p in [in_dir_path, out_dir_path]:
    if not p.is_dir():
      raise ValueError(f'{p} is Not a directory!')

  # Find utterences in input dir, output JSON to output dir
  for file in in_dir_path.iterdir():
    if '.TextGrid' not in file.name:
      continue
    grid = tg.TextGrid.fromFile(file)
    utterances = find_utterances(grid, normalise=normalise)
    out_path = out_dir_path / file.name.replace('TextGrid', 'json')
    write_to_json(utterances, out_path)

if __name__ == '__main__':
  main()
  print('Done!')

